{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45b6cc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python for beginners\n",
      "Python Web Development\n",
      "Python Machine Learning\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open('home.html', 'r') as html_file:\n",
    "    content = html_file.read()\n",
    "\n",
    "    # content of the html file, parser method we want to use\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    tags = soup.find('h5')  # searches for the first element\n",
    "    courses_html_tags = soup.find_all('h5')\n",
    "\n",
    "    # Display all the courses\n",
    "    for course in courses_html_tags:\n",
    "        # We'll get only the text for each element in the list returned by soup.find_all\n",
    "        print(course.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "43028cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The course Python for beginners costs 20$\n",
      "The course Python Web Development costs 50$\n",
      "The course Python Machine Learning costs 100$\n"
     ]
    }
   ],
   "source": [
    "# Grab the prices\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open ('home.html','r') as html_file:\n",
    "    content = html_file.read()\n",
    "    # print(content)\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    # class is a built-in method in Python, that's why we need to use class_\n",
    "    course_cards = soup.find_all('div',class_='card' )\n",
    "    for course in course_cards:\n",
    "        course_name = course.h5.text\n",
    "        course_price = course.a.text.split()[-1] # to access the last element, which will be the price\n",
    "        \n",
    "        # course.a or course.h5 can access different tags inside a div and returns a tag\n",
    "        \n",
    "        print(f\"The course {course_name} costs {course_price}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99fc6aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method_descriptor:\n",
      "\n",
      "split(self, /, sep=None, maxsplit=-1) unbound builtins.str method\n",
      "    Return a list of the substrings in the string, using sep as the separator string.\n",
      "\n",
      "      sep\n",
      "        The separator used to split the string.\n",
      "\n",
      "        When set to None (the default value), will split on any whitespace\n",
      "        character (including \\n \\r \\t \\f and spaces) and will discard\n",
      "        empty strings from the result.\n",
      "      maxsplit\n",
      "        Maximum number of splits.\n",
      "        -1 (the default value) means no limit.\n",
      "\n",
      "    Splitting starts at the front of the string and works to the end.\n",
      "\n",
      "    Note, str.split() is mainly useful for data that has been intentionally\n",
      "    delimited.  With natural text that includes punctuation, consider using\n",
      "    the regular expression module.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(str.split))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459d9d6b",
   "metadata": {},
   "source": [
    "-----BeautifulSoup constructor-----\n",
    "\n",
    "\n",
    "Parameters:\n",
    "markup (required):\n",
    "The string or file-like object containing the HTML or XML content you want to parse. This can be:\n",
    "\n",
    "A string of HTML or XML\n",
    "\n",
    "An open file or URL (with .read() called or passed directly)\n",
    "\n",
    "parser (optional but recommended):\n",
    "The parser you want BeautifulSoup to use. Common choices:\n",
    "\n",
    "\"html.parser\" — built-in Python HTML parser (default)\n",
    "\n",
    "\"lxml\" — faster, requires the lxml library\n",
    "\n",
    "\"xml\" — for parsing XML with lxml\n",
    "\n",
    "\"html5lib\" — parses like a web browser, very lenient, requires html5lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7944c9b",
   "metadata": {},
   "source": [
    "listings is a ResultSet (a special list-like object from BeautifulSoup).\n",
    "\n",
    "Each item in listings (e.g., listing) is a Tag.\n",
    "\n",
    "Tag objects have the .find(), .find_all(), .text, etc. methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d238440",
   "metadata": {},
   "source": [
    ".get() to access any attribute of a Tag element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d038cfe",
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFeatureNotFound\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     42\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m------------------------------------------------------\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[43mfind_jobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mfind_jobs\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfind_jobs\u001b[39m():\n\u001b[32m      7\u001b[39m     html_text = requests.get(\n\u001b[32m      8\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mhttps://m.timesjobs.com/mobile/jobs-search-result.html?txtKeywords=python&cboWorkExp1=-1&txtLocation=\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     soup = \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhtml_text\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlxml\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     listings = soup.find_all(\u001b[33m'\u001b[39m\u001b[33mdiv\u001b[39m\u001b[33m'\u001b[39m, class_=\u001b[33m'\u001b[39m\u001b[33msrp-listing\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m listing \u001b[38;5;129;01min\u001b[39;00m listings:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\Desktop\\Projects\\WebScrap\\.venv\\Lib\\site-packages\\bs4\\__init__.py:364\u001b[39m, in \u001b[36mBeautifulSoup.__init__\u001b[39m\u001b[34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[39m\n\u001b[32m    362\u001b[39m     possible_builder_class = builder_registry.lookup(*features)\n\u001b[32m    363\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m possible_builder_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m FeatureNotFound(\n\u001b[32m    365\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find a tree builder with the features you \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    366\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mrequested: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m. Do you need to install a parser library?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    367\u001b[39m             % \u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m.join(features)\n\u001b[32m    368\u001b[39m         )\n\u001b[32m    369\u001b[39m     builder_class = possible_builder_class\n\u001b[32m    371\u001b[39m \u001b[38;5;66;03m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[32m    372\u001b[39m \u001b[38;5;66;03m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[32m    373\u001b[39m \u001b[38;5;66;03m# with the remaining **kwargs.\u001b[39;00m\n",
      "\u001b[31mFeatureNotFound\u001b[39m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "%pip install lxml\n",
    "\n",
    "# Scrape real websites using requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from datetime import datetime\n",
    "def find_jobs():\n",
    "    html_text = requests.get(\n",
    "        'https://m.timesjobs.com/mobile/jobs-search-result.html?txtKeywords=python&cboWorkExp1=-1&txtLocation=')\n",
    "    soup = BeautifulSoup(html_text.text, 'lxml')\n",
    "    listings = soup.find_all('div', class_='srp-listing')\n",
    "\n",
    "\n",
    "    for listing in listings:\n",
    "        job_link = listing.find('a').get('href')\n",
    "        html_job_text = requests.get(job_link).text\n",
    "        inner_soup = BeautifulSoup(html_job_text, 'lxml')\n",
    "        outer_infos = inner_soup.find('div', class_=['jd-page', 'ui-page', 'ui-page-theme-a',\n",
    "                                                    'ui-page-header-fixed', 'ui-page-footer-fixed', 'ui-page-active'])\n",
    "        if outer_infos:\n",
    "            inner_infos = outer_infos.find(\n",
    "                'div', class_='jdpage-main')\n",
    "            # Get the location and the years of experience\n",
    "            location_exp_infos = inner_infos.find('div', class_='clearfix exp-loc')\n",
    "            location_list = location_exp_infos.find(\n",
    "                'div', class_='srp-loc jd-loc').text.split()\n",
    "\n",
    "            print(location_list[1].strip('()/') +\n",
    "                ' - ' + location_list[2].strip('()/'))\n",
    "\n",
    "            years_of_experience = location_exp_infos.find(\n",
    "                'div', class_='srp-exp').text.split()\n",
    "            print(years_of_experience[0] + ' ' + years_of_experience[1])\n",
    "\n",
    "            # Get the job title, the company and the day the job listing was posted\n",
    "            job_information = inner_infos.find('div', id='jobTitle')\n",
    "            job_title = job_information.h1.text\n",
    "            company_name = job_information.h2.span.text\n",
    "            posting_time = job_information.find('span', class_='posting-time').text\n",
    "            posting_time_date = datetime.strptime(posting_time,'%d %b, %Y').date() #b means abbreviated time\n",
    "            # posting_time_date = posting_time_date.strftime(\"%d-%m-%Y\")\n",
    "            print(job_title + \" \"+company_name+\" \"+str(posting_time_date))\n",
    "            print('------------------------------------------------------')\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    find_jobs()\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
