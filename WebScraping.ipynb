{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45b6cc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python for beginners\n",
      "Python Web Development\n",
      "Python Machine Learning\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open('home.html', 'r') as html_file:\n",
    "    content = html_file.read()\n",
    "\n",
    "    # content of the html file, parser method we want to use\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    tags = soup.find('h5')  # searches for the first element\n",
    "    courses_html_tags = soup.find_all('h5')\n",
    "\n",
    "    # Display all the courses\n",
    "    for course in courses_html_tags:\n",
    "        # We'll get only the text for each element in the list returned by soup.find_all\n",
    "        print(course.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "43028cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The course Python for beginners costs 20$\n",
      "The course Python Web Development costs 50$\n",
      "The course Python Machine Learning costs 100$\n"
     ]
    }
   ],
   "source": [
    "# Grab the prices\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open ('home.html','r') as html_file:\n",
    "    content = html_file.read()\n",
    "    # print(content)\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    # class is a built-in method in Python, that's why we need to use class_\n",
    "    course_cards = soup.find_all('div',class_='card' )\n",
    "    for course in course_cards:\n",
    "        course_name = course.h5.text\n",
    "        course_price = course.a.text.split()[-1] # to access the last element, which will be the price\n",
    "        \n",
    "        # course.a or course.h5 can access different tags inside a div and returns a tag\n",
    "        \n",
    "        print(f\"The course {course_name} costs {course_price}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99fc6aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method_descriptor:\n",
      "\n",
      "split(self, /, sep=None, maxsplit=-1) unbound builtins.str method\n",
      "    Return a list of the substrings in the string, using sep as the separator string.\n",
      "\n",
      "      sep\n",
      "        The separator used to split the string.\n",
      "\n",
      "        When set to None (the default value), will split on any whitespace\n",
      "        character (including \\n \\r \\t \\f and spaces) and will discard\n",
      "        empty strings from the result.\n",
      "      maxsplit\n",
      "        Maximum number of splits.\n",
      "        -1 (the default value) means no limit.\n",
      "\n",
      "    Splitting starts at the front of the string and works to the end.\n",
      "\n",
      "    Note, str.split() is mainly useful for data that has been intentionally\n",
      "    delimited.  With natural text that includes punctuation, consider using\n",
      "    the regular expression module.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(str.split))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459d9d6b",
   "metadata": {},
   "source": [
    "-----BeautifulSoup constructor-----\n",
    "\n",
    "\n",
    "Parameters:\n",
    "markup (required):\n",
    "The string or file-like object containing the HTML or XML content you want to parse. This can be:\n",
    "\n",
    "A string of HTML or XML\n",
    "\n",
    "An open file or URL (with .read() called or passed directly)\n",
    "\n",
    "parser (optional but recommended):\n",
    "The parser you want BeautifulSoup to use. Common choices:\n",
    "\n",
    "\"html.parser\" — built-in Python HTML parser (default)\n",
    "\n",
    "\"lxml\" — faster, requires the lxml library\n",
    "\n",
    "\"xml\" — for parsing XML with lxml\n",
    "\n",
    "\"html5lib\" — parses like a web browser, very lenient, requires html5lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7944c9b",
   "metadata": {},
   "source": [
    "listings is a ResultSet (a special list-like object from BeautifulSoup).\n",
    "\n",
    "Each item in listings (e.g., listing) is a Tag.\n",
    "\n",
    "Tag objects have the .find(), .find_all(), .text, etc. methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d238440",
   "metadata": {},
   "source": [
    ".get() to access any attribute of a Tag element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d038cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found job: Python Developer at zenga tv posted on 2025-08-02\n",
      "Found job: Python Developer at LAKSH HUMAN RESOURCE posted on 2025-08-02\n",
      "Found job: Developer - Python at Wipro Technologies Ltd posted on 2025-07-31\n",
      "Found job: Python Developer at Infocom Software posted on 2025-07-27\n",
      "Found job: Python Developer at Hotelogix posted on 2025-07-26\n",
      "Found job: Python Developer at Infocom Software posted on 2025-07-27\n",
      "Found job: Python Developer at SYNECHRON posted on 2025-07-25\n",
      "Found job: Python Developer at SEVEN CONSULTANCY posted on 2025-07-25\n",
      "Found job: Python Developer at SYNECHRON posted on 2025-07-24\n",
      "Found job: Python Developer at IQVIA posted on 2025-07-24\n",
      "Found job: Python Developer at SEVEN CONSULTANCY posted on 2025-07-25\n",
      "Found job: Python Developer at ORACLE posted on 2025-07-23\n",
      "Found job: Python Developer at SYNECHRON posted on 2025-07-24\n",
      "Found job: Python Developer at Techasoft Pvt Ltd posted on 2025-07-20\n",
      "Found job: Python Developer at SEVEN CONSULTANCY posted on 2025-07-20\n",
      "Found job: Python Developer at o9 Solutions, Inc. posted on 2025-07-22\n",
      "Found job: Python Developer at Ctd Techs Private Limited posted on 2025-07-20\n",
      "Found job: Python Developer at zucol services posted on 2025-07-22\n",
      "Found job: Python Developers at botree technologies posted on 2025-07-20\n",
      "Found job: Python Developer at AxisTechnolabs posted on 2025-07-20\n",
      "Found job: Python Trainer at Excel Ptp posted on 2025-07-20\n",
      "Found job: Python Developer at highrise solutions llp posted on 2025-07-20\n",
      "Found job: Python Programmer at 3RI Technologies Pvt Ltd posted on 2025-07-20\n",
      "Found job: Python Developer at ADROIT LEARNING AND MANPOWER PVT LTD posted on 2025-07-20\n",
      "Found job: Python Developer at CGI Information Systems and Management Consultants Pvt Ltd posted on 2025-07-22\n",
      "\n",
      "Saved 25 jobs to jobs_2025-08-04.csv\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_page(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    return BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "def parse_job_links(soup, limit=5):\n",
    "    # Extract the links to the job pages from the main page\n",
    "    listings = soup.find_all('div', class_='srp-listing')\n",
    "    if limit == -1:\n",
    "        links = [listing.find('a').get('href') for listing in listings]\n",
    "    else:\n",
    "        links = [listing.find('a').get('href') for listing in listings[:limit]]\n",
    "    return links\n",
    "\n",
    "def parse_job_details(job_url):\n",
    "    # Parse the page of a job and extract the useful details\n",
    "    soup = fetch_page(job_url)\n",
    "    outer_infos = soup.find('div', class_=['jd-page', 'ui-page', 'ui-page-theme-a',\n",
    "                                           'ui-page-header-fixed', 'ui-page-footer-fixed', 'ui-page-active'])\n",
    "    if not outer_infos:\n",
    "        return None\n",
    "    \n",
    "    inner_infos = outer_infos.find('div', class_='jdpage-main')\n",
    "    \n",
    "    # Job info: title, company and the positing date\n",
    "    job_information = inner_infos.find('div', id='jobTitle')\n",
    "    job_title = job_information.h1.text.strip()\n",
    "    company_name = job_information.h2.span.text.strip()\n",
    "    posting_time = job_information.find('span', class_='posting-time').text.strip()\n",
    "    posting_time_date = datetime.strptime(posting_time, '%d %b, %Y').date()\n",
    "    \n",
    "    # Location and experience\n",
    "    location_exp_infos = inner_infos.find('div', class_='clearfix exp-loc')\n",
    "    location_text = location_exp_infos.find('div', class_='srp-loc jd-loc').text.strip()\n",
    "    location_list = location_text.split()\n",
    "    location = location_list[1].translate(str.maketrans('', '', '()/,')) + ' - ' + location_list[2].translate(str.maketrans('', '', '()/,'))\n",
    "\n",
    "    \n",
    "    years_of_experience = location_exp_infos.find('div', class_='srp-exp').text.split()\n",
    "    years_of_experience = years_of_experience[0] + ' '+ years_of_experience[1 ]\n",
    "    \n",
    "    return {\n",
    "        'Title': job_title,\n",
    "        'Company': company_name,\n",
    "        'Posted on': str(posting_time_date),\n",
    "        'Location': location,\n",
    "        'Experience': years_of_experience\n",
    "    }\n",
    "\n",
    "def find_jobs():\n",
    "    base_url = 'https://m.timesjobs.com/mobile/jobs-search-result.html?txtKeywords=python&cboWorkExp1=-1&txtLocation='\n",
    "    soup = fetch_page(base_url)\n",
    "    job_links = parse_job_links(soup, limit=-1)\n",
    "    \n",
    "    jobs = []\n",
    "    for link in job_links:\n",
    "        try:\n",
    "            job = parse_job_details(link)\n",
    "            if job:\n",
    "                print(f\"Found job: {job['Title']} at {job['Company']} posted on {job['Posted on']}\")\n",
    "                jobs.append(job)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to parse job at {link}: {e}\")\n",
    "    \n",
    "    if jobs:\n",
    "        df = pd.DataFrame(jobs)\n",
    "        csv_file = f\"jobs_{datetime.today().strftime('%Y-%m-%d')}.csv\"\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        print(f\"\\nSaved {len(jobs)} jobs to {csv_file}\")\n",
    "    else:\n",
    "        print(\"No jobs found.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    find_jobs()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
